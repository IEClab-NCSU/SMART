# Few shot learning and fine-tuning GPT models

## Description
Code in the gpt directory includes experiments for few-shot learning and fine-tuning GPT models to generate skill labels.  
    
## Table of Contents
  * [Inputs](#inputs)
  * [Outputs](#outputs)
  * [Running the Experiment](#running-the-experiment)

## Inputs
The inputs required for the comparison study include:

- The output file generated by the [parser](../OLI%20XML%20Parser/README.md) for each course:
  - Assessments (assessments.csv)   
- The files with expert mapping of skills to assessment items from DataShop. For the OLI Introduction to Biology and OLI General Chemistry 1 courses, the files are on OneDrive at the `PASTEL Project/SMART/DataShop Export/OLI Biology/Skills` and `PASTEL Project/SMART/DataShop Export/OLI General Chemistry I/Skills` filepaths.
  - For OLI Introduction to Biology:
    - intro_bio_skill_map.tsv
    - intro_bio_skills.tsv
  - For OLI General Chemistry 1:
    - chem1_skill_map_2.3_5_20_20-Problems.tsv
    - chem1_skill_map_2.3_5_20_20-Skills.tsv

## Outputs
The `process_data.py` script generates three output directories:
- `Dataframes`
    The pickle files for the pandas dataframes for each dataset are stored at this location.
- `TrainingDatasets`
    The text files for training datasets for input into the GPT model are stored at this location.
- `ValidationDatasets`
    The text files for validation and test datasets for hyperparameter selection and evaluation of the GPT models are stored at this location.

The `run_finetuning.sh` script generates one output directory:
- `FineTunedModels`
    The files created when saving the fine-tuned models (including checkpoints) are saved in this directory.

The `few_shot_keyphrase_generation_gpt2.py` script generates two output directories:
- `gpt_j_evaluation_few_shot`
    A .csv file is stored at this location with the evaluation results (precision, recall, and f1) for few-shot learning on the OLI datasets.
- `gpt_j_output_few_shot`
    Files named `few_shot_gpt_j_<course>_<number_of_shots>_shots_output.csv` contain the original document, target skill labels, and generated skill labels output     by GPT during the experiment.

The `keyphrase_generation_gpt_oli.py` script generates two output directories:
- `gpt_output_oli`
    Files named `<train_val_or_test>_<course>_steps_<number_of_steps>.csv` contain the original document, target skill labels, and generated skill labels output     by GPT during the experiment.
- `gpt_evaluation_oli
    A .csv file is stored at this location with the evaluation results (precision, recall, and f1) for fine-tuning GPT on the OLI datasets.`

The `keyphrase_generation_oli_evaluation.py` script generates two output directories:
- `gpt_output_oli_test`
    Files named `<train_val_or_test>_<course>_steps_<number_of_steps>.csv` contain the original document, target skill labels, and generated skill labels output       by GPT during the experiment.
- `gpt_evaluation_oli_test`
    A .csv file is stored at this location with the evaluation results (precision, recall, and f1) for fine-tuning GPT on the OLI datasets.`

The `keyphrase_generation_gpt_oli.py` script generates two output directories:
- `gpt_output_public`
    Files named `<train_val_or_test>_<course>_steps_<number_of_steps>.csv` contain the original document, target skill labels, and generated skill labels output       by GPT during the experiment.
- `gpt_evaluation_public`
    A .csv file is stored at this location with the evaluation results (precision, recall, and f1) for fine-tuning GPT on the public datasets.`

The `keyphrase_generation_gpt_public.py` script generates two output directories:
- `gpt_output_public_test`
    Files named `<train_val_or_test>_<course>_steps_<number_of_steps>.csv` contain the original document, target skill labels, and generated skill labels output       by GPT during the experiment.
- `gpt_evaluation_public_test`
    A .csv file is stored at this location with the evaluation results (precision, recall, and f1) for fine-tuning GPT on the public datasets.`

## Running the Experiment
### Setup
To format and save the necessary files for input into the GPT model, use the following command:
```
python process_data.py
```
`process_data.py` reads in the datasets and saves the pickle files for the pandas dataframes to the `Dataframes` folder. Also, the OLI datasets are split into train, validation, and test sets (10%/10%/80%), formatted into a text file for use by GPT, and saved to the `TrainingDatasets` and `ValidationDatasets` folders. Finally, the kdd and inspec datasets are combined and split into train, validation, and test sets (80%/10%/10%), formatted into a text file for use by GPT, and saved to the `TrainingDatasets` and `ValidationDatasets` folders. The full OLI datasets are also formatted into a text file for use by GPT and saved in the `ValidationDatasets` folder for use with few-shot GPT.

### Few-Shot Learning
To run the experiment with few shot learning, use the following command on the terminal and evaluation results will be output to the console:
```
python few_shot_keyphrase_generation_gpt2.py
```
### Fine-tuning
First, the GPT model must be fine-tuned using the OLI training data for the respective course or the training data from the public datasets (Inspec and KDD). To do so, use the following command:
```
bash run_finetuning.sh
```
(Note: Expect the fine-tuning process to be lengthy (3-4 days), especially for using the public datasets.)

To run the experiment to optimize the number of training steps for fine-tuning the GPT model with 10% OLI training data, use the following command:
```
python keyphrase_generation_gpt_oli.py
```

To run the experiment to evaluate on 10% OLI test data after fine-tuning the GPT model with 10% OLI training data and 10% OLI validation data, use the following command:
```
python keyphrase_generation_oli_evaluation.py
```
(Note: Update the optimal number of training steps based on the learning curve for each course in line 37.)

To run the experiment to optimize the number of training steps for fine-tuning the GPT model with 80% public training data (Inspec and KDD) and 10% public validation data, use the following command:
```
python keyphrase_generation_gpt_public.py
```

To run the experiment to evaluate on 10% public test data after fine-tuning the GPT model with 80% public training data (Inspec and KDD) and 10% public validation data, use the following command:
```
python keyphrase_generation_public_evaluation.py
```
(Note: Update the optimal number of training steps based on the learning curve in line 37.)
(Note: The experiment currently evaluates the generatlity of the GPT model fine-tuned on public data on a test set of public data. Future modification is required to evaluate the ability of the model for transfer learning i.e. performance on OLI datasets.)
