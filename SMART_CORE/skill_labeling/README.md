# Exploration of Skill Labeling

## Description
The skill labeling section of SMART-CORE examines several approaches for using keyphrase extraction or generation to create skill labels for the clusters of assessment items.

## Table of Contents
  * [Overview](#overview)
  * [Comparing Unsupervised Automatic Keyphrase Extraction Algorithms](#comparing-unsupervised-automatic-keyphrase-extraction-algorithms)
    + [Inputs](#inputs)
    + [Outputs](#outputs)
    + [Running the Study](#running-the-study)

## Overview

1. Code in the `skill_labeling` directory runs a comparison study on several unsupervised keyphrase extraction algorithms applied to the OLI Introduction to Biology and OLI General Chemistry 1 courses to extract skill labels.  
    
    The algorithms are evaluated using partial precision, partial recall, and partial f1 score, where the skill labels from the expert skill model in DataShop is used as ground truth.  
    
    The algorithms evaluated are RAKE, YAKE, TextRank, SingleRank, TopicRank, MultipartiteRank, and KeyBERT.
2. Code in the `embedding_ake_study` sub-directory runs an experiment to determine the appropriate choice for candidate keyphrase generation, document embeddings, and ranking/selection for embedding-based keyphrase extraction.  

    The options for candidate keyphrase generation are:
    - n-grams: use the unigrams, bigrams, and trigrams from the processed document as candidate keyphrases.
    - parsing: parse out noun phrases composed of adjectives and nouns as candidate keyphrases.  
    
    The options for document embeddings are:
    - Word2vec
    - GloVE
    - Doc2vec
    - Sentence-BERT

    The options for ranking and selection are:
    - top-n: Take the top n candidates by cosine similarity between the candidate embedding and document embedding.
    - top-n-modified: Take the top n candidates by score, where score is the sum of the cosine similarity between terms in the candidate and the document.
    - mmr: Take the top n candidates by score after applying Maximal Marginal Relevance (MMR) 
    - mss:  Take the top n candidates by score after applying Maximum Sum Similarity (MSS)  
  3. Code in the `gpt` sub-directory includes experiments for few-shot learning and fine-tuning GPT models to generate skill labels.

## Comparing Unsupervised Automatic Keyphrase Extraction Algorithms

### Inputs
The inputs required for the comparison study include:

- The output files generated by the [parser](../OLI%20XML%20Parser/README.md) for each course:
  - Assessments (assessments.csv)  
  - Paragraphs (paragraphs.csv)  
- The files with expert mapping of skills to assessment items from DataShop. For the OLI Introduction to Biology and OLI General Chemistry 1 courses, the files are on OneDrive at the `PASTEL Project/SMART/DataShop Export/OLI Biology/Skills` and `PASTEL Project/SMART/DataShop Export/OLI General Chemistry I/Skills` filepaths.
  - For OLI Introduction to Biology:
    - intro_bio_skill_map.tsv
    - intro_bio_skills.tsv
  - For OLI General Chemistry 1:
    - chem1_skill_map_2.3_5_20_20-Problems.tsv
    - chem1_skill_map_2.3_5_20_20-Skills.tsv

### Outputs
The study generates three output directories:

- `AKE_Result_Dataframes` 
The pickle files for the pandas dataframe with the original document, extracted skill label, and target skill label for each algorithm on each course.

- `detailed_performance`  
A .csv file named like `<course>_<algorithm>_performance_categorization.csv` (e.g., `oli-intro-bio_keybert_performance_categorization.csv`) that includes a column with a categorization for each observation (skill label) based on its individual performance:
  - 1: Reasonable Precision and Reasonable Recall
  - 2: Reasonable Precision and Poor Recall
  - 3: Poor Precision and Reasonable Recall
  - 4: Poor Precision and Poor Recall  
  
  (Note: Poor performance indicates a value below 0.66 and reasonable performance indicates a value above 0.66).
  
A pdf file named `target_keyphrase_histogram.pdf` with a histogram showing the distribution for the ratio of the number of target keywords not found in the original document to number of target keywords.

- `keyphrase_comparison`
  For each dataset, contains three files:
    - A file named `<dataset>_best_worst_predictions.csv` that lists the index of the best and worst extracted keyphrases for each algorithm on the dataset according to F1.
    - A file named `<dataset>_keyphrase_comparison_filtered.csv` that displays the target and extracted skill labels across all algorithms if the skill label appeared as a best or worst skill label for any single algorithm on the dataset.
    - A file named `<dataset>_keyphrase_comparison_full.csv` that displays the target and extracted skill labels across all algorithms for all documents in the dataset.

## Running the Study
To run the study, use the following command on the terminal and evaluation results will be output to the console:
```
python extract_skill_labels.py
```

To run additional analysis of the results, use the following command on the terminal:
```
python assess_precision_recall_v2.py
```
To run the comparison of keyphrases extracted by the various algorithms, use the following command on the terminal:
```
python compare_extracted_keyphrases.py
```
